{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Main Multiome Notebook**\n\nThis is the main notebook for multiome part of the project, where the task is to predict gene expression levels given information about TF_IDF normalized chromatin accessibility data.\n\nTarget includes 23000 genes. Kaggle notebook can hardly fit all the target values in available memory, and there is no possibility to fit an individual model for every of 23000 targets. So, I calculate TruncatedSVD components for the target data, predict the TruncatedSVD components and then calculate the predicted targets by using reverse operation to TruncatedSVD calculation. To further improve results, I build 4 models predicting TruncatedSVD components calculated with different random seeds and then calculate the average prediction.\n\nIn this Jupyter notebook, data from several sources is joined together and is used further to create predictions for the test dataset. The sources are:\n\n* Pre-calculated Truncated SVD values from chromatin accessibility data (see Prepare_SVD_for_multiome notebook).\n* Source data for three input features to be used as is.\n* Metadata - donor ID and day each cell was analyzed, few features are built using metadata information.\n* Target values for the train set.","metadata":{}},{"cell_type":"code","source":"# Importing the libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport gc, pickle, scipy.sparse\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom humanize import naturalsize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Need this libraby to read the *.h5 data\n!pip install --quiet tables","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:00:09.222866Z","iopub.execute_input":"2022-11-15T16:00:09.223258Z","iopub.status.idle":"2022-11-15T16:00:21.535045Z","shell.execute_reply.started":"2022-11-15T16:00:09.223225Z","shell.execute_reply":"2022-11-15T16:00:21.533938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/open-problems-multimodal/\"\nFP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n\nFP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\nFP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\nFP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\nFP_CITE_TEST_INPUTS_FIX = os.path.join(DATA_DIR,\"test_cite_inputs_day_2_donor_27678.h5\")\n\nFP_MULTIOME_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\nFP_MULTIOME_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\nFP_MULTIOME_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n\nFP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\nFP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:00:28.323465Z","iopub.execute_input":"2022-11-15T16:00:28.323919Z","iopub.status.idle":"2022-11-15T16:00:28.332882Z","shell.execute_reply.started":"2022-11-15T16:00:28.323877Z","shell.execute_reply":"2022-11-15T16:00:28.331865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import specially prepared TruncatedSVD data and select rows related to train data.\n# Only 128 components will be used, as cross-validation showed other components add little value to the model.\nsvd_x = pd.read_csv('../input/raw-features-for-multiome/svd.csv', dtype='float32')\nsvd_x = svd_x.iloc[:105942, :129]\n#svd_x = svd_x.iloc[:105942]\nsvd_x = svd_x.add_prefix('svd_x_')\ndel svd_x['svd_x_Unnamed: 0']\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:06:41.622049Z","iopub.execute_input":"2022-11-15T16:06:41.623271Z","iopub.status.idle":"2022-11-15T16:06:55.462648Z","shell.execute_reply.started":"2022-11-15T16:06:41.623216Z","shell.execute_reply":"2022-11-15T16:06:55.461542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get column names from target data.\ndf_target = pd.read_hdf(FP_MULTIOME_TRAIN_TARGETS, start=0, stop=1)\ntarget_names = df_target.columns\n\ndel df_target\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:07:00.733400Z","iopub.execute_input":"2022-11-15T16:07:00.734594Z","iopub.status.idle":"2022-11-15T16:07:01.145344Z","shell.execute_reply.started":"2022-11-15T16:07:00.734543Z","shell.execute_reply":"2022-11-15T16:07:01.144201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Import prepared sparse matrix of target values.\n\ntrain_targets = scipy.sparse.load_npz(\"../input/multimodal-single-cell-as-sparse-matrix/train_multi_targets_values.sparse.npz\")","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:07:04.959268Z","iopub.execute_input":"2022-11-15T16:07:04.960193Z","iopub.status.idle":"2022-11-15T16:07:31.190662Z","shell.execute_reply.started":"2022-11-15T16:07:04.960149Z","shell.execute_reply":"2022-11-15T16:07:31.188895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_pca(name, model):\n    with open(name, 'wb') as f:\n        pickle.dump(model, f)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:07:57.888779Z","iopub.execute_input":"2022-11-15T16:07:57.889528Z","iopub.status.idle":"2022-11-15T16:07:57.894494Z","shell.execute_reply.started":"2022-11-15T16:07:57.889483Z","shell.execute_reply":"2022-11-15T16:07:57.893659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For targets, I calculate the TruncatedSVD components in the main notebook and use pickle to save the model, \n# so that it would be possible to perform reverse operation later.\n# To achieve better results, I calculate the TruncatedSVD components 4 times and will later fit 4 models\n# and calculate the average.\nfor i in [2,3,4,5]:\n    file_name = 'pca_targets_' + str(i) + '.pkl'\n    prefix = 'svd_y_' + str(i) + '_'\n    pca_targets = TruncatedSVD(n_components=64, random_state=i)\n    #pca_targets = TruncatedSVD(n_components=4, random_state=i)\n    t_targets = pca_targets.fit_transform(train_targets)\n    save_pca(file_name, pca_targets)\n    target_i = pd.DataFrame(t_targets, dtype='float32')\n    target_i = target_i.add_prefix(prefix)\n    if i == 2:\n        target_total = target_i\n    else:\n        target_total = pd.concat([target_total, target_i], axis=1)\n\n    \ndel t_targets, train_targets, target_i\ngc.collect()\nprint(target_total.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:15:42.426748Z","iopub.execute_input":"2022-11-15T16:15:42.427195Z","iopub.status.idle":"2022-11-15T16:19:12.163850Z","shell.execute_reply.started":"2022-11-15T16:15:42.427160Z","shell.execute_reply":"2022-11-15T16:19:12.162496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Import metadata and select rows related to train set.\nmd_df = pd.read_csv(FP_CELL_METADATA, index_col='cell_id')\nmd_df = md_df.loc[md_df['technology'] == \"multiome\"]\nmd_df['day'] = md_df['day'].astype('int8')\ndel md_df['technology']\nmd_df = md_df.loc[(md_df['donor'] != 27678) & (md_df['day'] != 10)]\nprint(md_df.shape)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:20:15.403281Z","iopub.execute_input":"2022-11-15T16:20:15.404057Z","iopub.status.idle":"2022-11-15T16:20:15.982830Z","shell.execute_reply.started":"2022-11-15T16:20:15.404014Z","shell.execute_reply":"2022-11-15T16:20:15.981578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import pre-selected important features to be used as is.\ndf_imp_cols = pd.read_parquet('../input/imp-features-for-multiome/train_corr_features.parquet')\nvery_imp_cols = ['svd_x_chr1:630875-631689', 'svd_x_chr1:633700-634539', 'svd_x_chr17:22520955-22521852']\ndf_imp_cols = df_imp_cols[very_imp_cols]\nprint(df_imp_cols.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:20:22.492642Z","iopub.execute_input":"2022-11-15T16:20:22.493057Z","iopub.status.idle":"2022-11-15T16:20:24.163761Z","shell.execute_reply.started":"2022-11-15T16:20:22.493023Z","shell.execute_reply":"2022-11-15T16:20:24.162508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now join all the train data into a single dataframe.\nmd_df = md_df.merge(df_imp_cols, how = 'left', on = 'cell_id')\ndf = md_df.reset_index()\ndf = pd.concat([df, svd_x], axis=1)\ndf = pd.concat([df, target_total], axis=1)\nprint(df.shape)\n\n\ndel md_df, svd_x, target_total, df_imp_cols\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:20:29.723446Z","iopub.execute_input":"2022-11-15T16:20:29.723852Z","iopub.status.idle":"2022-11-15T16:20:29.967105Z","shell.execute_reply.started":"2022-11-15T16:20:29.723818Z","shell.execute_reply":"2022-11-15T16:20:29.965950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the dataframe size.\nsize = df.memory_usage(deep='True').sum()\nprint(size)\nprint(naturalsize(size))","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:20:36.908075Z","iopub.execute_input":"2022-11-15T16:20:36.908484Z","iopub.status.idle":"2022-11-15T16:20:36.957682Z","shell.execute_reply.started":"2022-11-15T16:20:36.908445Z","shell.execute_reply":"2022-11-15T16:20:36.956349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now import the prepared TruncatedSVD data for test dataset.\nsvd_test = pd.read_csv('../input/raw-features-for-multiome/svd.csv', dtype='float32')\nsvd_test = svd_test.iloc[105942:, :129]\nsvd_test = svd_test.add_prefix('svd_x_')\ndel svd_test['svd_x_Unnamed: 0']\nsvd_test = svd_test.reset_index(drop = True)\nprint(svd_test.shape)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:21:01.897222Z","iopub.execute_input":"2022-11-15T16:21:01.897637Z","iopub.status.idle":"2022-11-15T16:21:09.003293Z","shell.execute_reply.started":"2022-11-15T16:21:01.897601Z","shell.execute_reply":"2022-11-15T16:21:09.001973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import metadata for test dataset.\nmd_df = pd.read_csv(FP_CELL_METADATA, index_col='cell_id')\nmd_df = md_df.loc[md_df['technology'] == \"multiome\"]\nmd_df['day'] = md_df['day'].astype('int8')\ndel md_df['technology']\nmd_df = md_df.loc[(md_df['donor'] == 27678) | (md_df['day'] == 10)]\nprint(md_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:21:14.043958Z","iopub.execute_input":"2022-11-15T16:21:14.044358Z","iopub.status.idle":"2022-11-15T16:21:14.379652Z","shell.execute_reply.started":"2022-11-15T16:21:14.044326Z","shell.execute_reply":"2022-11-15T16:21:14.378539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import data for pre-selected important features (test dataset).\ndf_imp_cols = pd.read_parquet('../input/imp-features-for-multiome/test_corr_features.parquet')\nvery_imp_cols = ['svd_x_chr1:630875-631689', 'svd_x_chr1:633700-634539', 'svd_x_chr17:22520955-22521852']\ndf_imp_cols = df_imp_cols[very_imp_cols]\nprint(df_imp_cols.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:23:01.915169Z","iopub.execute_input":"2022-11-15T16:23:01.915574Z","iopub.status.idle":"2022-11-15T16:23:02.772884Z","shell.execute_reply.started":"2022-11-15T16:23:01.915543Z","shell.execute_reply":"2022-11-15T16:23:02.772021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now join all the test data into a single dataframe.\nmd_df = md_df.merge(df_imp_cols, how = 'left', on = 'cell_id')\ndf_test = md_df.reset_index()\ndf_test = pd.concat([df_test, svd_test], axis=1)\nprint(df_test.shape)\ndel md_df, svd_test, df_imp_cols\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:23:09.554462Z","iopub.execute_input":"2022-11-15T16:23:09.555308Z","iopub.status.idle":"2022-11-15T16:23:09.730146Z","shell.execute_reply.started":"2022-11-15T16:23:09.555269Z","shell.execute_reply":"2022-11-15T16:23:09.728995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_params_submit_fast = {\n    \"learning_rate\" : 0.06,\n    \"eval_metric\" : 'RMSE', \n    \"max_depth\" : 7,\n    \"verbose\" : 100,\n    \"n_estimators\" : 800,\n    \"task_type\" : 'GPU'\n    }\ncat_params_submit_middle = {\n    \"learning_rate\" : 0.04,\n    \"eval_metric\" : 'RMSE', \n    \"max_depth\" : 7,\n    \"verbose\" : 100,\n    #\"reg_lambda\" : 20,\n    \"n_estimators\" : 600,\n    \"task_type\" : 'GPU'\n    }\ncat_params_submit_slow = {\n    \"learning_rate\" : 0.03,\n    \"eval_metric\" : 'RMSE', \n    \"max_depth\" : 7,\n    \"verbose\" : 100,\n    #\"reg_lambda\" : 20,\n    \"n_estimators\" : 400,\n    \"task_type\" : 'GPU'\n    }","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:24:38.287190Z","iopub.execute_input":"2022-11-15T16:24:38.287566Z","iopub.status.idle":"2022-11-15T16:24:38.294875Z","shell.execute_reply.started":"2022-11-15T16:24:38.287534Z","shell.execute_reply":"2022-11-15T16:24:38.293474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create  metadata features both for test and train.\n# Note: here I cannot use \"get_dummies\" because one of the donors is only present in test set.\ndef add_metadata_features(d_frame):\n    d_frame['svd_x_donor_13176'] = 0\n    d_frame['svd_x_donor_31800'] = 0\n    d_frame['svd_x_donor_32606'] = 0\n    d_frame.loc[d_frame['donor'] == 13176, 'svd_x_donor_13176'] = 1\n    d_frame.loc[d_frame['donor'] == 31800, 'svd_x_donor_31800'] = 1\n    d_frame.loc[d_frame['donor'] == 32606, 'svd_x_donor_32606'] = 1\n    d_frame['svd_x_day'] = d_frame['day']\n    return d_frame","metadata":{"execution":{"iopub.status.busy":"2022-11-15T16:24:41.827198Z","iopub.execute_input":"2022-11-15T16:24:41.827845Z","iopub.status.idle":"2022-11-15T16:24:41.833572Z","shell.execute_reply.started":"2022-11-15T16:24:41.827809Z","shell.execute_reply":"2022-11-15T16:24:41.832664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building catboost models and predicting target TruncatedSVD components in a cycle.\n# Note that I use stronger parameters for the first TruncatedSVD components.\n# For the last components I use fewer iterations and smaller learning rate to prevent overfitting.\ndf = add_metadata_features(df)\ndf_test = add_metadata_features(df_test)\nx_cols = [col for col in list(df.columns) if (col.startswith('svd_x_'))]\ny_cols = [col for col in list(df.columns) if (col.startswith('svd_y_'))]\nX = df[x_cols].values\nY = df[y_cols].values\nXt = df_test[x_cols].values\nfor i in range(len(y_cols)):\n    print('Training_column: ' + str(i))\n    num = int(y_cols[i].rsplit('_', 1)[-1])\n    #model = lightgbm.LGBMRegressor(**lightgbm_params)\n    #model = CatBoostRegressor(**cat_params_submit)\n    if num < 16:\n        model = CatBoostRegressor(**cat_params_submit_fast)\n    elif num < 32:\n        model = CatBoostRegressor(**cat_params_submit_middle)\n    else:\n        model = CatBoostRegressor(**cat_params_submit_slow)\n    model.fit(X, Y[:,i].copy())\n    col_name = y_cols[i]\n    df_test[col_name] = model.predict(Xt)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df, X, Y, Xt, model\nfor col in df_test.columns:\n    if col in x_cols:\n        del df_test[col]\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the final results.\ndf_test[y_cols].reset_index().to_feather('multiome_multi.ftr')","metadata":{},"execution_count":null,"outputs":[]}]}