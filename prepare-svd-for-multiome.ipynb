{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Prepare SVD for multiome**\n\nIn this Jupyter notebook, data from train and test datasets is put together and then the TruncatedSVD is calculated. This is done twice: once for data normalized by organizers, and then for raw data. Only SVD features made from normalized data were used in a final submission.\n\nIn kaggle environment it is more convenient to do this in a separate notebook, as it would be a waste of both time and GPU quota to calculate the TruncatedSVD each time before fitting the model.","metadata":{}},{"cell_type":"code","source":"# Importing the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport gc, pickle, scipy.sparse\nfrom humanize import naturalsize\nfrom sklearn.decomposition import TruncatedSVD\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-23T18:28:33.745752Z","iopub.execute_input":"2022-11-23T18:28:33.746190Z","iopub.status.idle":"2022-11-23T18:28:35.089343Z","shell.execute_reply.started":"2022-11-23T18:28:33.746106Z","shell.execute_reply":"2022-11-23T18:28:35.088304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Need this library to read *.h5 files\n!pip install --quiet tables","metadata":{"execution":{"iopub.status.busy":"2022-11-23T18:28:37.043201Z","iopub.execute_input":"2022-11-23T18:28:37.043605Z","iopub.status.idle":"2022-11-23T18:28:49.225305Z","shell.execute_reply.started":"2022-11-23T18:28:37.043570Z","shell.execute_reply":"2022-11-23T18:28:49.224213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/open-problems-multimodal/\"\nFP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n\nFP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\nFP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\nFP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\n\nFP_MULTIOME_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\nFP_MULTIOME_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\nFP_MULTIOME_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n\nFP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\nFP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-23T18:28:52.799750Z","iopub.execute_input":"2022-11-23T18:28:52.800142Z","iopub.status.idle":"2022-11-23T18:28:52.808337Z","shell.execute_reply.started":"2022-11-23T18:28:52.800110Z","shell.execute_reply":"2022-11-23T18:28:52.807134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The multiome train dataset raw data is too large to be loaded into RAM. But it is also sparse.\n# So, I load the dataset in chunks, and then convert it to sparse matrix.\n# Will use this function to do right that.\n\n\ndef read_convert_hdf_in_chunks(link, chunk_size, sparse_matrice=None):\n    i = 0\n    while i < 1000000:\n        df_chunk = pd.read_hdf(link, start=i, stop=i+chunk_size)\n        sparse_chunk = scipy.sparse.csr_matrix(df_chunk.values)\n        if sparse_matrice == None:\n            sparse_matrice = sparse_chunk\n        else:\n            sparse_matrice = scipy.sparse.vstack([sparse_matrice, sparse_chunk])\n        print(i)\n        i += chunk_size\n        if sparse_chunk.shape[0] < chunk_size:\n            return sparse_matrice\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-23T18:28:56.080480Z","iopub.execute_input":"2022-11-23T18:28:56.080980Z","iopub.status.idle":"2022-11-23T18:28:56.094516Z","shell.execute_reply.started":"2022-11-23T18:28:56.080938Z","shell.execute_reply":"2022-11-23T18:28:56.093081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Loading raw data inputs\n\nsparse_X = read_convert_hdf_in_chunks('../input/open-problems-raw-counts/train_multi_inputs_raw.h5', 5000)\nprint(sparse_X.shape[0])\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-23T18:29:18.821685Z","iopub.execute_input":"2022-11-23T18:29:18.822075Z","iopub.status.idle":"2022-11-23T18:29:26.948901Z","shell.execute_reply.started":"2022-11-23T18:29:18.822043Z","shell.execute_reply":"2022-11-23T18:29:26.947683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Same procedure for the test raw data.\nsparse_X = read_convert_hdf_in_chunks('/kaggle/input/open-problems-raw-counts/test_multi_inputs_raw.h5', 5000, sparse_X)\nprint(sparse_X.shape[0])\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-23T18:02:49.813595Z","iopub.execute_input":"2022-11-23T18:02:49.814422Z","iopub.status.idle":"2022-11-23T18:14:41.308626Z","shell.execute_reply.started":"2022-11-23T18:02:49.814377Z","shell.execute_reply":"2022-11-23T18:14:41.306763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export total_counts. Maybe they will be useful as a feature.\ntotal_counts = sparse_X.sum(axis=1)\ncounts_index = [*range(len(total_counts))]\ntotal_counts = total_counts.flat\ndf_total_counts = pd.DataFrame({'total_counts': total_counts}, index=counts_index)\ndf_total_counts.to_feather('total_counts_multiome.ftr')","metadata":{"execution":{"iopub.status.busy":"2022-11-09T13:38:14.235805Z","iopub.execute_input":"2022-11-09T13:38:14.236295Z","iopub.status.idle":"2022-11-09T13:38:14.373717Z","shell.execute_reply.started":"2022-11-09T13:38:14.236256Z","shell.execute_reply":"2022-11-09T13:38:14.372751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Apply the singular value decomposition.\n\nprint(f\"Shape of both before SVD: {sparse_X.shape}\")\nsvd = TruncatedSVD(n_components=64, random_state=1)\nsparse_X = svd.fit_transform(sparse_X)\nprint(f\"Shape of both after SVD:  {sparse_X.shape}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save results in a file.\ndf_svd = pd.DataFrame(sparse_X)\ndf_svd.to_csv('svd_raw.csv')\nprint('Raw data SVD ready')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Free the RAM.\ndel sparse_X, df_svd\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Generally the same operations for the normalized data using the same function.\n# Load the train data in chunks and convert it to sparse matrix.\n\nsparse_X = read_convert_hdf_in_chunks(FP_MULTIOME_TRAIN_INPUTS, 5000)\nprint(sparse_X.shape[0])\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-09T16:44:58.722655Z","iopub.execute_input":"2022-11-09T16:44:58.723193Z","iopub.status.idle":"2022-11-09T17:09:43.143500Z","shell.execute_reply.started":"2022-11-09T16:44:58.723153Z","shell.execute_reply":"2022-11-09T17:09:43.141885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Same for normalized test dataset.\n\nsparse_X = read_convert_hdf_in_chunks(FP_MULTIOME_TEST_INPUTS, 5000, sparse_X)\nprint(sparse_X.shape[0])\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Apply the singular value decomposition.\n# Normalized data is more important, so I will prepare more components.\n\nprint(f\"Shape of both before SVD: {sparse_X.shape}\")\nsvd = TruncatedSVD(n_components=256, random_state=1)\nsparse_X = svd.fit_transform(sparse_X)\nprint(f\"Shape of both after SVD:  {sparse_X.shape}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save results in a file.\ndf_svd = pd.DataFrame(sparse_X)\ndf_svd.to_csv('svd.csv')\nprint('All the SVD ready')","metadata":{},"execution_count":null,"outputs":[]}]}